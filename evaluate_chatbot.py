#!/usr/bin/env python3
"""
Chatbot Evaluation Script with Metrics
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

from empathetic_chat_advanced import AdvancedEmpatheticChatBot
import wandb
import os
from dotenv import load_dotenv
import time
import json
from typing import List, Dict
import numpy as np

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# è©•ä¾¡ç”¨ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
EVALUATION_TEST_CASES = [
    {
        "input": "ä»Šæ—¥ã¯ã¨ã¦ã‚‚ç–²ã‚Œã¾ã—ãŸã€‚ä»•äº‹ãŒå¤§å¤‰ã§...",
        "expected_keywords": ["ãŠç–²ã‚Œ", "å¤§å¤‰", "ä¼‘", "ç†è§£"],
        "category": "fatigue"
    },
    {
        "input": "å‹é”ã¨å–§å˜©ã—ã¦ã—ã¾ã£ã¦æ‚²ã—ã„ã§ã™",
        "expected_keywords": ["æ‚²ã—", "ç†è§£", "æ°—æŒã¡", "å‹"],
        "category": "relationship"
    },
    {
        "input": "ä»•äº‹ãŒä¸Šæ‰‹ãã„ã‹ãªãã¦è½ã¡è¾¼ã‚“ã§ã„ã¾ã™",
        "expected_keywords": ["è½ã¡è¾¼", "ä»•äº‹", "å¤§ä¸ˆå¤«", "æ”¯æ´"],
        "category": "work"
    },
    {
        "input": "æœ€è¿‘å¯ã‚Œãªãã¦è¾›ã„ã§ã™",
        "expected_keywords": ["è¾›", "ç¡çœ ", "ä¼‘", "å¿ƒé…"],
        "category": "health"
    },
    {
        "input": "è©¦é¨“ã«è½ã¡ã¦ã—ã¾ã„ã¾ã—ãŸ",
        "expected_keywords": ["è©¦é¨“", "æ¬¡", "é ‘å¼µ", "å¤§ä¸ˆå¤«"],
        "category": "failure"
    },
    {
        "input": "I'm feeling very anxious about my future",
        "expected_keywords": ["understand", "anxious", "future", "support"],
        "category": "anxiety"
    },
    {
        "input": "I feel so lonely these days",
        "expected_keywords": ["lonely", "understand", "care", "here"],
        "category": "loneliness"
    }
]

class ChatbotEvaluator:
    def __init__(self, model_name: str = "mistral-7b", use_wandb: bool = True):
        """
        è©•ä¾¡ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        """
        self.chatbot = AdvancedEmpatheticChatBot(
            model_name=model_name,
            use_wandb=use_wandb,
            project_name="chatbot-evaluation"
        )
        self.results = []
        self.use_wandb = use_wandb
    
    def evaluate_empathy_score(self, response: str, expected_keywords: List[str]) -> float:
        """
        å…±æ„Ÿåº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        """
        response_lower = response.lower()
        matched_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)
        score = matched_keywords / len(expected_keywords) if expected_keywords else 0
        return score
    
    def evaluate_response_quality(self, response: str) -> Dict:
        """
        å¿œç­”å“è³ªã®è©•ä¾¡
        """
        quality_metrics = {
            "length": len(response),
            "word_count": len(response.split()),
            "sentence_count": response.count('ã€‚') + response.count('.') + response.count('!') + response.count('?'),
            "has_greeting": any(greet in response.lower() for greet in ["ã“ã‚“ã«ã¡ã¯", "hello", "hi"]),
            "has_question": '?' in response or 'ï¼Ÿ' in response,
            "has_encouragement": any(enc in response.lower() for enc in ["é ‘å¼µ", "å¤§ä¸ˆå¤«", "ã§ãã‚‹", "you can", "it's okay", "will be"])
        }
        
        # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        quality_score = 0
        if 50 <= quality_metrics["length"] <= 500:
            quality_score += 0.25
        if quality_metrics["word_count"] >= 10:
            quality_score += 0.25
        if quality_metrics["has_question"]:
            quality_score += 0.25
        if quality_metrics["has_encouragement"]:
            quality_score += 0.25
        
        quality_metrics["quality_score"] = quality_score
        
        return quality_metrics
    
    def run_evaluation(self, test_cases: List[Dict] = None) -> Dict:
        """
        è©•ä¾¡ã®å®Ÿè¡Œ
        """
        if test_cases is None:
            test_cases = EVALUATION_TEST_CASES
        
        print("\n" + "="*60)
        print("ğŸ”¬ Starting Chatbot Evaluation")
        print(f"Model: {self.chatbot.model_name}")
        print(f"Test cases: {len(test_cases)}")
        print("="*60 + "\n")
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}] Testing: {test_case['category']}")
            print(f"Input: {test_case['input']}")
            
            # å¿œç­”ç”Ÿæˆ
            response, metrics = self.chatbot.generate_response(
                test_case['input'],
                max_length=300,
                temperature=0.7
            )
            
            print(f"Response: {response[:200]}..." if len(response) > 200 else f"Response: {response}")
            
            # è©•ä¾¡
            empathy_score = self.evaluate_empathy_score(response, test_case.get('expected_keywords', []))
            quality_metrics = self.evaluate_response_quality(response)
            
            # çµæœã®ä¿å­˜
            result = {
                "test_case": i,
                "category": test_case['category'],
                "input": test_case['input'],
                "response": response,
                "empathy_score": empathy_score,
                "response_time": metrics['response_time'],
                **quality_metrics,
                **metrics
            }
            
            self.results.append(result)
            
            print(f"âœ“ Empathy Score: {empathy_score:.2f}")
            print(f"âœ“ Quality Score: {quality_metrics['quality_score']:.2f}")
            print(f"âœ“ Response Time: {metrics['response_time']:.2f}s")
            print("-" * 40)
            
            # WandBã«ãƒ­ã‚°
            if self.use_wandb:
                wandb.log({
                    f"eval/{test_case['category']}/empathy_score": empathy_score,
                    f"eval/{test_case['category']}/quality_score": quality_metrics['quality_score'],
                    f"eval/{test_case['category']}/response_time": metrics['response_time']
                })
        
        # ç·åˆè©•ä¾¡
        overall_metrics = self.calculate_overall_metrics()
        
        return overall_metrics
    
    def calculate_overall_metrics(self) -> Dict:
        """
        å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        """
        if not self.results:
            return {}
        
        overall = {
            "total_tests": len(self.results),
            "avg_empathy_score": np.mean([r['empathy_score'] for r in self.results]),
            "avg_quality_score": np.mean([r['quality_score'] for r in self.results]),
            "avg_response_time": np.mean([r['response_time'] for r in self.results]),
            "avg_response_length": np.mean([r['length'] for r in self.results]),
            "avg_word_count": np.mean([r['word_count'] for r in self.results]),
            "std_response_time": np.std([r['response_time'] for r in self.results]),
            "min_response_time": min([r['response_time'] for r in self.results]),
            "max_response_time": max([r['response_time'] for r in self.results])
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é›†è¨ˆ
        categories = {}
        for result in self.results:
            cat = result['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(result)
        
        category_metrics = {}
        for cat, results in categories.items():
            category_metrics[cat] = {
                "avg_empathy": np.mean([r['empathy_score'] for r in results]),
                "avg_quality": np.mean([r['quality_score'] for r in results]),
                "avg_time": np.mean([r['response_time'] for r in results])
            }
        
        overall["category_metrics"] = category_metrics
        
        # WandBã«ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°
        if self.use_wandb:
            wandb.log({"overall": overall})
        
        return overall
    
    def save_results(self, filename: str = "evaluation_results.json"):
        """
        è©•ä¾¡çµæœã®ä¿å­˜
        """
        output = {
            "model": self.chatbot.model_name,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": self.results,
            "overall_metrics": self.calculate_overall_metrics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ Results saved to {filename}")
    
    def print_summary(self):
        """
        è©•ä¾¡ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        """
        metrics = self.calculate_overall_metrics()
        
        print("\n" + "="*60)
        print("ğŸ“Š EVALUATION SUMMARY")
        print("="*60)
        
        print(f"\nğŸ“ˆ Overall Metrics:")
        print(f"  â€¢ Total Tests: {metrics['total_tests']}")
        print(f"  â€¢ Avg Empathy Score: {metrics['avg_empathy_score']:.3f}")
        print(f"  â€¢ Avg Quality Score: {metrics['avg_quality_score']:.3f}")
        print(f"  â€¢ Avg Response Time: {metrics['avg_response_time']:.2f}s Â± {metrics['std_response_time']:.2f}s")
        print(f"  â€¢ Avg Response Length: {metrics['avg_response_length']:.0f} chars")
        
        print(f"\nğŸ“‚ Category Performance:")
        for cat, cat_metrics in metrics['category_metrics'].items():
            print(f"  {cat}:")
            print(f"    - Empathy: {cat_metrics['avg_empathy']:.3f}")
            print(f"    - Quality: {cat_metrics['avg_quality']:.3f}")
            print(f"    - Time: {cat_metrics['avg_time']:.2f}s")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        print(f"\nğŸ¯ Performance Rating:")
        overall_score = (metrics['avg_empathy_score'] + metrics['avg_quality_score']) / 2
        if overall_score >= 0.8:
            rating = "Excellent â­â­â­â­â­"
        elif overall_score >= 0.6:
            rating = "Good â­â­â­â­"
        elif overall_score >= 0.4:
            rating = "Fair â­â­â­"
        else:
            rating = "Needs Improvement â­â­"
        
        print(f"  Overall Score: {overall_score:.3f} - {rating}")
        print("="*60)

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate Empathetic ChatBot")
    parser.add_argument("--model", default="mistral-7b",
                       help="Model to evaluate")
    parser.add_argument("--no_wandb", action="store_true",
                       help="Disable WandB logging")
    parser.add_argument("--save", default="evaluation_results.json",
                       help="Output filename for results")
    
    args = parser.parse_args()
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    evaluator = ChatbotEvaluator(
        model_name=args.model,
        use_wandb=not args.no_wandb
    )
    
    # è©•ä¾¡å®Ÿè¡Œ
    overall_metrics = evaluator.run_evaluation()
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    evaluator.print_summary()
    
    # çµæœä¿å­˜
    evaluator.save_results(args.save)
    
    # WandBçµ‚äº†
    if evaluator.use_wandb:
        wandb.finish()

if __name__ == "__main__":
    main()