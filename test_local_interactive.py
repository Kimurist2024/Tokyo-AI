"""
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ã®å¯¾è©±ãƒ†ã‚¹ãƒˆï¼ˆOpenAI APIä¸è¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å¼ï¼‰
"""

import requests
import json
from typing import List, Dict

class LocalModelChat:
    def __init__(self, base_url="http://localhost:11434"):
        """Ollamaãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šã‚’åˆæœŸåŒ–"""
        self.base_url = base_url
        self.model_name = "llama3.2:3b"  # è»½é‡ãªæ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«
    
    def is_ollama_running(self) -> bool:
        """Ollamaã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def chat_with_local_model(self, messages: List[Dict[str, str]]) -> str:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆ"""
        
        if not self.is_ollama_running():
            return "âŒ Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚\nèµ·å‹•æ–¹æ³•:\n1. ollama serve\n2. ollama pull llama3.2:3b"
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµåˆ
        prompt = self.format_messages_for_ollama(messages)
        
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                return f"âŒ ã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except Exception as e:
            return f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}"
    
    def format_messages_for_ollama(self, messages: List[Dict[str, str]]) -> str:
        """OpenAIå½¢å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Ollamaç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›"""
        
        formatted_prompt = ""
        
        for message in messages:
            role = message["role"]
            content = message["content"]
            
            if role == "system":
                formatted_prompt += f"ã‚·ã‚¹ãƒ†ãƒ : {content}\n\n"
            elif role == "user":
                formatted_prompt += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {content}\n"
            elif role == "assistant":
                formatted_prompt += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {content}\n"
        
        formatted_prompt += "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
        return formatted_prompt

def interactive_chat():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«ã‚ˆã‚‹å¯¾è©±"""
    
    chat = LocalModelChat()
    
    print("=" * 60)
    print("ğŸ  ãƒ­ãƒ¼ã‚«ãƒ«LLM å¯¾è©±ãƒ†ã‚¹ãƒˆï¼ˆOpenAI APIä¸è¦ï¼‰")
    print("=" * 60)
    print("ğŸ’¬ ä½•ã§ã‚‚è³ªå•ã—ã¦ãã ã•ã„ï¼")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›")
    print("ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã«ã¯ 'reset' ã¨å…¥åŠ›")
    print("-" * 60)
    
    # åˆæœŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§ä¸å¯§ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚"
        }
    ]
    
    while True:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        user_input = input("\nğŸ‘¤ ã‚ãªãŸ: ").strip()
        
        # çµ‚äº†ãƒã‚§ãƒƒã‚¯
        if user_input.lower() in ['exit', 'quit', 'çµ‚äº†', 'bye']:
            print("\nğŸ‘‹ ãŠç–²ã‚Œã•ã¾ã§ã—ãŸï¼")
            break
        
        # ãƒªã‚»ãƒƒãƒˆãƒã‚§ãƒƒã‚¯
        if user_input.lower() in ['reset', 'ãƒªã‚»ãƒƒãƒˆ']:
            messages = [
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§ä¸å¯§ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚"
                }
            ]
            print("ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            continue
        
        # ç©ºå…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if not user_input:
            continue
        
        # ç¾åœ¨ã®ä¼šè©±ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
        current_messages = messages + [{"role": "user", "content": user_input}]
        
        print("\nğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«AI: ", end="", flush=True)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¿œç­”ã‚’å–å¾—
        response = chat.chat_with_local_model(current_messages)
        print(response)
        
        # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
        messages.extend([
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": response}
        ])
        
        # å±¥æ­´ãŒé•·ããªã‚Šã™ããŸã‚‰å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        if len(messages) > 11:  # ã‚·ã‚¹ãƒ†ãƒ  + 10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ã§
            messages = [messages[0]] + messages[-10:]

def show_setup_instructions():
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•ã‚’è¡¨ç¤º"""
    
    print("""
ğŸš€ Ollama ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•:

ã€1. Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‘
curl -fsSL https://ollama.com/install.sh | sh

ã€2. Ollamaã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã€‘
ollama serve

ã€3. è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‘
ollama pull llama3.2:3b        # ç´„2GB
# ã¾ãŸã¯
ollama pull qwen2:1.5b         # ç´„1.5GB (è»½ã„)
ollama pull phi3:mini          # ç´„2.3GB

ã€4. ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œã€‘
python test_local_interactive.py

ğŸ’¡ ãƒ’ãƒ³ãƒˆ:
- Ollamaã¯åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ 'ollama serve' ã§èµ·å‹•
- ãƒ¢ãƒ‡ãƒ«ã¯åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šä¸è¦ã§å‹•ä½œï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œï¼‰
""")

if __name__ == "__main__":
    print("ğŸ  ãƒ­ãƒ¼ã‚«ãƒ«LLM å¯¾è©±ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("OpenAI APIã‚­ãƒ¼ã¯ä¸è¦ã§ã™")
    
    # ã¾ãšOllamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
    chat = LocalModelChat()
    
    if chat.is_ollama_running():
        print("âœ… Ollamaæ¥ç¶šOK - å¯¾è©±é–‹å§‹")
        interactive_chat()
    else:
        print("âš ï¸  OllamaãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
        show_setup_instructions()
        
        choice = input("\nOllamaã‚’èµ·å‹•æ¸ˆã¿ã®å ´åˆã€å¯¾è©±ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if choice == 'y':
            interactive_chat()