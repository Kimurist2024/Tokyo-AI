#!/usr/bin/env python3
"""
Ollama Empathetic ChatBot with WandB Metrics
Ollamaã‚’ä½¿ã£ãŸå…±æ„Ÿçš„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼ˆå®Œå…¨ç„¡æ–™ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
"""

import requests
import json
import time
import argparse
import wandb
import os
from dotenv import load_dotenv
from typing import Dict, List, Tuple

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

class OllamaEmpatheticChatBot:
    def __init__(self, 
                 model: str = "llama3.2:3b",
                 base_url: str = "http://localhost:11434",
                 use_wandb: bool = True,
                 project_name: str = "ollama-empathetic-chatbot"):
        """
        Ollamaã‚’ä½¿ã£ãŸå…±æ„Ÿçš„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®åˆæœŸåŒ–
        
        Args:
            model: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«
            base_url: Ollamaã‚µãƒ¼ãƒãƒ¼ã®URL
            use_wandb: WandBã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
            project_name: WandBãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
        """
        self.model = model
        self.base_url = base_url
        
        # WandBã®åˆæœŸåŒ–
        if use_wandb:
            self._init_wandb(project_name, model)
        self.use_wandb = use_wandb
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.system_prompt = """You are an empathetic AI assistant who provides emotional support and understanding. Your responses should be:

1. Warm, caring, and supportive
2. Non-judgmental and accepting of all feelings
3. Focused on active listening and reflection
4. Culturally sensitive and appropriate
5. Encouraging when appropriate
6. Responsive in the same language as the user

Guidelines:
- Use empathetic language like "I understand", "That sounds difficult", "Your feelings are valid"
- Ask gentle follow-up questions to show engagement
- Keep responses concise but meaningful (2-4 sentences)
- Offer practical suggestions only when appropriate
- Always maintain a caring, professional tone

Remember: Your goal is to help people feel heard, understood, and emotionally supported."""
        
        # ä¼šè©±å±¥æ­´
        self.conversation_history = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.conversation_metrics = {
            "response_times": [],
            "response_lengths": [],
            "user_inputs": [],
            "bot_responses": []
        }
        
        # Ollamaã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
        self._test_connection()
        
        print(f"âœ… Ollama ChatBot initialized with model: {model}")
    
    def _init_wandb(self, project_name: str, model: str):
        """WandBã®åˆæœŸåŒ–"""
        try:
            wandb.login(key=os.getenv("WANDB_API_KEY"))
            wandb.init(
                project=project_name,
                config={
                    "model": model,
                    "provider": "ollama",
                    "framework": "ollama",
                    "task": "empathetic_conversation"
                }
            )
            print("âœ… WandB initialized successfully!")
        except Exception as e:
            print(f"âš ï¸  Could not initialize WandB: {e}")
            self.use_wandb = False
    
    def _test_connection(self):
        """Ollamaã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… Ollama server connection successful")
                
                # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model not in model_names:
                    print(f"âš ï¸  Model '{self.model}' not found. Available models: {model_names}")
                    print(f"To download: ollama pull {self.model}")
                else:
                    print(f"âœ… Model '{self.model}' is available")
            else:
                print(f"âŒ Ollama server returned status {response.status_code}")
        except requests.exceptions.ConnectionError:
            print("âŒ Cannot connect to Ollama server. Please run 'ollama serve'")
        except Exception as e:
            print(f"âŒ Connection test failed: {e}")
    
    def generate_response(self, user_input: str, temperature: float = 0.7) -> Tuple[str, Dict]:
        """
        Ollamaã‚’ä½¿ã£ã¦å…±æ„Ÿçš„ãªå¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        start_time = time.time()
        
        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        self.conversation_history.append({"role": "user", "content": user_input})
        
        try:
            # Ollama APIã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": self.conversation_history,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "top_p": 0.9
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                bot_response = result['message']['content'].strip()
                
                # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                self.conversation_history.append({"role": "assistant", "content": bot_response})
                
                # å¿œç­”æ™‚é–“è¨ˆç®—
                response_time = time.time() - start_time
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                metrics = {
                    "response_time": response_time,
                    "response_length": len(bot_response),
                    "response_words": len(bot_response.split()),
                    "input_length": len(user_input),
                    "input_words": len(user_input.split())
                }
                
                # å…±æ„Ÿåº¦ã‚¹ã‚³ã‚¢
                empathy_score = self._calculate_empathy_score(bot_response)
                metrics["empathy_score"] = empathy_score
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
                self.conversation_metrics["response_times"].append(response_time)
                self.conversation_metrics["response_lengths"].append(len(bot_response))
                self.conversation_metrics["user_inputs"].append(user_input)
                self.conversation_metrics["bot_responses"].append(bot_response)
                
                # WandBã«ãƒ­ã‚°
                if self.use_wandb:
                    wandb.log(metrics)
                
                return bot_response, metrics
                
            else:
                error_msg = f"Ollama API error: {response.status_code}"
                error_metrics = {
                    "error": True,
                    "response_time": time.time() - start_time,
                    "response_length": len(error_msg),
                    "response_words": len(error_msg.split())
                }
                return error_msg, error_metrics
                
        except requests.exceptions.ConnectionError:
            error_msg = "Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚'ollama serve'ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            error_metrics = {
                "error": True,
                "response_time": time.time() - start_time,
                "response_length": len(error_msg),
                "response_words": len(error_msg.split())
            }
            return error_msg, error_metrics
            
        except Exception as e:
            error_msg = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
            error_metrics = {
                "error": True,
                "response_time": time.time() - start_time,
                "response_length": len(error_msg),
                "response_words": len(error_msg.split())
            }
            return error_msg, error_metrics
    
    def _calculate_empathy_score(self, response: str) -> float:
        """å…±æ„Ÿåº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        empathetic_keywords = [
            # è‹±èª
            "understand", "feel", "sorry", "hear", "support", "care", "help",
            "difficult", "challenging", "valid", "important", "here for you",
            # æ—¥æœ¬èª
            "ç†è§£", "æ„Ÿã˜", "ã¤ã‚‰ã„", "å¤§å¤‰", "æ”¯æ´", "ã‚µãƒãƒ¼ãƒˆ", "èã",
            "æ°—æŒã¡", "å¿ƒé…", "å¤§ä¸ˆå¤«", "é ‘å¼µ", "å¿œæ´"
        ]
        
        response_lower = response.lower()
        matched_keywords = sum(1 for keyword in empathetic_keywords 
                             if keyword.lower() in response_lower)
        
        # è³ªå•ã®å­˜åœ¨ã‚‚ãƒã‚§ãƒƒã‚¯
        has_question = '?' in response or 'ï¼Ÿ' in response
        question_bonus = 0.1 if has_question else 0
        
        # 0-1ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§æ­£è¦åŒ–
        empathy_score = min(1.0, (matched_keywords * 0.1) + question_bonus)
        return empathy_score
    
    def chat_loop(self):
        """å¯¾è©±ãƒ«ãƒ¼ãƒ—"""
        print("\n" + "="*70)
        print("ğŸ¤– Ollama Empathetic ChatBot (Complete Free & Local)")
        print(f"Model: {self.model}")
        print("Type 'quit' or 'exit' to end the conversation")
        print("Type 'metrics' to see conversation statistics")
        print("Type 'models' to see available models")
        print("="*70 + "\n")
        
        while True:
            try:
                user_input = input("ğŸ’­ You: ")
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ Thank you for the conversation!")
                    self._print_session_summary()
                    break
                
                if user_input.lower() == 'metrics':
                    self._print_current_metrics()
                    continue
                    
                if user_input.lower() == 'models':
                    self._show_available_models()
                    continue
                
                if not user_input.strip():
                    continue
                
                print("ğŸ¤– Bot: ", end="", flush=True)
                
                response, metrics = self.generate_response(user_input)
                print(response)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                print(f"\nğŸ“Š [{metrics['response_time']:.2f}s | "
                      f"{metrics['response_words']} words | "
                      f"Empathy: {metrics.get('empathy_score', 0):.2f}]")
                print()
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Chat interrupted. Goodbye!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}")
    
    def _show_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print("\nğŸ“‹ Available Ollama Models:")
                for model in models:
                    name = model['name']
                    size = model.get('size', 0)
                    size_mb = size / (1024*1024) if size > 0 else 0
                    print(f"  â€¢ {name} ({size_mb:.1f}MB)")
                print()
            else:
                print("âŒ Could not retrieve model list")
        except Exception as e:
            print(f"âŒ Error getting models: {e}")
    
    def _print_current_metrics(self):
        """ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º"""
        if not self.conversation_metrics["response_times"]:
            print("ğŸ“Š No metrics available yet.")
            return
        
        total_conversations = len(self.conversation_metrics["response_times"])
        avg_response_time = sum(self.conversation_metrics["response_times"]) / total_conversations
        avg_response_length = sum(self.conversation_metrics["response_lengths"]) / total_conversations
        
        print("\nğŸ“Š Current Session Metrics:")
        print(f"  â€¢ Conversations: {total_conversations}")
        print(f"  â€¢ Avg Response Time: {avg_response_time:.2f}s")
        print(f"  â€¢ Avg Response Length: {avg_response_length:.0f} chars")
        print()
    
    def _print_session_summary(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®ã‚µãƒãƒªãƒ¼"""
        if not self.conversation_metrics["response_times"]:
            return
        
        total_conversations = len(self.conversation_metrics["response_times"])
        total_time = sum(self.conversation_metrics["response_times"])
        
        print("\n" + "="*50)
        print("ğŸ“ˆ SESSION SUMMARY")
        print("="*50)
        print(f"Total Conversations: {total_conversations}")
        print(f"Total Time: {total_time:.2f}s")
        print(f"Avg Time/Response: {total_time/total_conversations:.2f}s")
        print("="*50)

def main():
    parser = argparse.ArgumentParser(description="Ollama Empathetic ChatBot")
    parser.add_argument("--model", default="llama3.2:3b",
                       help="Ollama model to use (e.g., llama3.2:3b, llama3.1:8b, elyza:jp-8b)")
    parser.add_argument("--url", default="http://localhost:11434",
                       help="Ollama server URL")
    parser.add_argument("--no_wandb", action="store_true",
                       help="Disable WandB logging")
    parser.add_argument("--project", default="ollama-empathetic-chatbot",
                       help="WandB project name")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="Temperature for response generation")
    
    args = parser.parse_args()
    
    try:
        chatbot = OllamaEmpatheticChatBot(
            model=args.model,
            base_url=args.url,
            use_wandb=not args.no_wandb,
            project_name=args.project
        )
        
        # å¯¾è©±é–‹å§‹
        chatbot.chat_loop()
        
        # WandBçµ‚äº†
        if chatbot.use_wandb:
            wandb.finish()
            
    except Exception as e:
        print(f"âŒ Error initializing chatbot: {e}")
        print("Please ensure Ollama is running with 'ollama serve'")

if __name__ == "__main__":
    main()